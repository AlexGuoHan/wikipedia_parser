{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "test__file = '/home/ubuntu/wikipedia/DATA/OUTFILE/predicted_enwiki-20161201-pages-meta-history11.xml-p003046514p003201200.7z.tsv'\n",
    "data = pd.read_csv(test__file, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class DATA:\n",
    "the class data needs to provide following functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`buildCorpora()`** extracting unique words from user specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def buildCorpora(data, text_cols):\n",
    "    assert [col in data.columns for col in text_cols]\n",
    "    \n",
    "    corpora = set()\n",
    "    for col in text_cols:\n",
    "        _corpora = _extractUniqueWordsFromColumn(data, col)\n",
    "        corpora = corpora.union(_corpora)\n",
    "    return corpora\n",
    "        \n",
    "def _extractUniqueWordsFromColumn(data, text_col):\n",
    "    _data = data[text_col]\n",
    "    unique_words = set()\n",
    "    punc_table = str.maketrans({key: None for key in string.punctuation})\n",
    "    \n",
    "    for item in _data.items():\n",
    "        text = item[1]\n",
    "        if(type(text) is not str):\n",
    "            continue\n",
    "        \n",
    "        # remove punctuations\n",
    "        text = text.translate(punc_table)\n",
    "        for word in text.split():\n",
    "            if len(word) > 50:\n",
    "                # eg. \"http ... \"\n",
    "                pass\n",
    "            elif word not in unique_words:\n",
    "                unique_words.add(word)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return unique_words        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Will Multiprocessing Help Extracting Unique Words?\n",
    "Conclusion: the time taken is too small for multiprocessing to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words1 = _extractUniqueWordsFromColumn(data, 'Added')\n",
    "words2 = _extractUniqueWordsFromColumn(data, 'Deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: buildCorpora\n",
    "Except the new version excludes a few unnecessary tokens. The rest is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_groundtruth = []\n",
    "with open('test__corpus.txt') as f:\n",
    "    for line in f:\n",
    "        words_groundtruth.append(line.strip('\\n'))\n",
    "words_groundtruth = set(words_groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = buildCorpora(data, ['Added', 'Deleted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words_groundtruth.difference(words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`getWordVectors()`** extract words vectors using Facebook's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def computeVectors(source_file, mode='', identifier='', ft_dir='.', bin_dir='.'):\n",
    "    assert os.path.exists(ft_dir)\n",
    "    assert os,path.exists(bin_dir)\n",
    "    assert mode in ['word', 'sentence']\n",
    "    \n",
    "    ft_dir = os.path.join(ft_dir, 'fastText', 'fasttext')\n",
    "    bin_dir = os.path.join(bin_dir, 'wiki.en.bin')\n",
    "    target_dir = '__vectors_%s.txt'%identifier\n",
    "    func = 'print-%s-vectors'%(mode)\n",
    "    \n",
    "    # save the file to txt\n",
    "    if mode is 'word':\n",
    "        source_dir = '__source_file_%s.txt'%identifier\n",
    "        assert type(source_file) is set\n",
    "        with open(source_dir,'w') as file:\n",
    "            file.writelines([\"%s\\n\" % item  for item in source_file])\n",
    "    if mode is 'sentence':\n",
    "        source_dir = '__source_file_%s.csv'%identifier\n",
    "        assert type(source_file) is pd.core.series.Series\n",
    "        source_file.to_csv(source_dir, index=False)\n",
    "            \n",
    "    # compute the word vectors\n",
    "    command = \"sh -c \\'%s %s %s < %s > %s\\' \"%(ft_dir, \n",
    "                                                    func, \n",
    "                                                    bin_dir, \n",
    "                                                    source_dir, \n",
    "                                                    target_dir)\n",
    "    \n",
    "    print('Copy files to UNC server')\n",
    "    print('scp -i ~/alexguo.pem',\n",
    "          'ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/%s'%source_dir,\n",
    "          '.')\n",
    "    print('\\n Execute the files')\n",
    "    print(command)\n",
    "    print('\\n Copy them back')\n",
    "    print('scp -i ~/alexguo.pem',\n",
    "          '%s'%target_dir, \n",
    "          'ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/%s'%target_dir)\n",
    "    print('\\n Excute in AWS')\n",
    "    print('sudo mv ~/%s .'%target_dir)\n",
    "    \n",
    "    print('Please Use UNC Server')\n",
    "    \n",
    "    # subprocess.call(command, shell=True)\n",
    "    \n",
    "    # load them back\n",
    "    #lookup_dict = loadWordVectors(file=target_dir)    \n",
    "\n",
    "    #return lookup_dict\n",
    "    \n",
    "    \n",
    "def loadVectors(file, mode, dim=300):\n",
    "    assert mode in ['word', 'sentence']\n",
    "    lookup_dict = {}\n",
    "    \n",
    "    if mode is 'word':\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                key = line.split()[0]\n",
    "                values = line.split()[-dim:]\n",
    "                values = [float(v) for v in values]\n",
    "                lookup_dict[key] = np.array(values)\n",
    "                \n",
    "    if mode is 'sentence':\n",
    "        with open(file) as f:\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                key = idx\n",
    "                values = line.split()[-dim:]\n",
    "                \n",
    "                if values[0] in ['-nan' or 'nan']: \n",
    "                    lookup_dict[key] = np.zeros([dim])\n",
    "                else:\n",
    "                    values = [float(v) for v in values]\n",
    "                    lookup_dict[key] = np.array(values)\n",
    "                    \n",
    "                idx += 1\n",
    "            \n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: get/loadWordVectors\n",
    "It seems like the official release of `wiki.en.bin` has changed. This is confirmed by running fasttext on command line. The word vector for 'a' was changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy files to UNC server\n",
      "scp -i ~/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/__source_file_.txt .\n",
      "\n",
      " Execute the files\n",
      "sh -c './fastText/fasttext print-word-vectors ./wiki.en.bin < __source_file_.txt > __vectors_.txt' \n",
      "\n",
      " Copy them back\n",
      "scp -i ~/alexguo.pem __vectors_.txt ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/__vectors_.txt\n",
      "\n",
      " Excute in AWS\n",
      "sudo mv ~/__vectors_.txt .\n",
      "Please Use UNC Server\n",
      "CPU times: user 3.46 s, sys: 88 ms, total: 3.55 s\n",
      "Wall time: 3.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordvecs_groundtruth = loadVectors('test__wordvecs.txt', mode='word')\n",
    "computeVectors(words, mode='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvecs = loadVectors('__vectors.txt', mode='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:\n",
      "[ 0.11559    0.30192   -0.11465    0.01001   -0.032187  -0.10755    0.060674\n",
      " -0.10477    0.17488    0.0081116]\n",
      "\n",
      "\n",
      "Predicted\n",
      "[-0.02581     0.023828   -0.0094851   0.034731    0.017378    0.00047618\n",
      " -0.0075925  -0.068494    0.041394   -0.0015672 ]\n"
     ]
    }
   ],
   "source": [
    "print('Ground Truth:')\n",
    "print(wordvecs_groundtruth.get('a')[:10])\n",
    "print('\\n\\nPredicted')\n",
    "print(wordvecs.get('a')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`_computeSentenceVectorsUncombined()`** extract words vectors using Facebook's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _computeSentenceVectorsUncombined(data, text_cols):\n",
    "    assert [col in data.columns for col in text_cols]\n",
    "    \n",
    "    for col in text_cols:\n",
    "        texts = data[col]\n",
    "        # replace nan with ''\n",
    "        texts_cleaned = texts.apply(lambda x: \"\" if type(x) is not str else x)\n",
    "        # remove empty lines within one row\n",
    "        texts_cleaned = texts_cleaned.apply(lambda x: x.replace('\\n', ''))\n",
    "        computeVectors(texts_cleaned, mode='sentence', identifier=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: _computeSentenceVectorsUncombined\n",
    "Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy files to UNC server\n",
      "scp -i ~/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/__source_file_Added.csv .\n",
      "\n",
      " Execute the files\n",
      "sh -c './fastText/fasttext print-sentence-vectors ./wiki.en.bin < __source_file_Added.csv > __vectors_Added.txt' \n",
      "\n",
      " Copy them back\n",
      "scp -i ~/alexguo.pem __vectors_Added.txt ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/__vectors_Added.txt\n",
      "\n",
      " Excute in AWS\n",
      "sudo mv ~/__vectors_Added.txt .\n",
      "Please Use UNC Server\n",
      "Copy files to UNC server\n",
      "scp -i ~/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/__source_file_Deleted.csv .\n",
      "\n",
      " Execute the files\n",
      "sh -c './fastText/fasttext print-sentence-vectors ./wiki.en.bin < __source_file_Deleted.csv > __vectors_Deleted.txt' \n",
      "\n",
      " Copy them back\n",
      "scp -i ~/alexguo.pem __vectors_Deleted.txt ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/__vectors_Deleted.txt\n",
      "\n",
      " Excute in AWS\n",
      "sudo mv ~/__vectors_Deleted.txt .\n",
      "Please Use UNC Server\n"
     ]
    }
   ],
   "source": [
    "ca = _computeSentenceVectorsUncombined(data, ['Added'])\n",
    "cd = _computeSentenceVectorsUncombined(data, ['Deleted'])\n",
    "sentenceVec_a = loadVectors('__vectors_Added.txt', mode='sentence')\n",
    "sentenceVec_d = loadVectors('__vectors_Deleted.txt', mode='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceVec_d.get(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* **`tsneDimReduction()`** dimensionality reduction using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "def tsneDimReduction(data, new_dim=2):\n",
    "    model = TSNE(n_jobs=16, n_components=new_dim)\n",
    "    vec_tsne = model.fit_transform(data)\n",
    "    return vec_tsne\n",
    "\n",
    "def to_csv(data, filedir, drop_text=True):\n",
    "    if drop_text is True:\n",
    "        _data = data.drop('text', axis=1)\n",
    "        _data.to_csv(filedir)\n",
    "    else:\n",
    "        data.to_csv(filedir)\n",
    "    \n",
    "    print('scp -i ~/Desktop/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/%s ~/Documents/UNC/Research'%filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: tsneDimReduction\n",
    "passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictDF = pd.DataFrame.from_dict(sentenceVec, orient='index')\n",
    "sentenceVec_TSNE = tsneDimReduction(dictDF.values)\n",
    "data['sentenceVecDim1'] = sentenceVec_TSNE[:,0]\n",
    "data['sentenceVecDim2'] = sentenceVec_TSNE[:,1]\n",
    "to_csv(data, filedir='test__sentenceVec.csv', drop_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp -i ~/Desktop/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/test__sentenceVec.csv ~/Documents/UNC/Research\n"
     ]
    }
   ],
   "source": [
    "to_csv(data, filedir='test__sentenceVec.csv', drop_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "* **`computeSentenceVectors()`** combine wordvectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSentenceVectors(data, dim=300):\n",
    "    assert [os.path.exists('_vectors_%s.txt'%f) for f in ['Added', 'Deleted']]\n",
    "    assert 'title' in data.columns\n",
    "    \n",
    "    sentenceVec_Added = loadVectors('__vectors_Added.txt', mode='sentence')\n",
    "    sentenceVec_Deleted = loadVectors('__vectors_Deleted.txt', mode='sentence')\n",
    "    \n",
    "    title_prev = ''\n",
    "    sentenceVectors = np.zeros([data.shape[0], dim])\n",
    "    sentenceVectorsCentered = np.zeros([data.shape[0], dim])\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        title = row[1]['title']\n",
    "        idx = row[0]\n",
    "        delta_vector = sentenceVec_Added.get(idx) - sentenceVec_Deleted.get(idx)\n",
    "\n",
    "        if title is not title_prev:\n",
    "            sentenceVectors[idx, :] = delta_vector\n",
    "            sentenceVectorsCentered[idx, :] = np.zeros([dim])\n",
    "        if title is title_prev:\n",
    "            sentenceVectors[idx, :] = sentenceVectors[idx-1, :] + delta_vector\n",
    "            sentenceVectorsCentered[idx, :] = sentenceVectorsCentered[idx-1, :] + delta_vector\n",
    "            \n",
    "        title_prev = title\n",
    "    \n",
    "    sentenceVectors_TSNE = tsneDimReduction(sentenceVectors)\n",
    "    sentenceVectorsCentered_TSNE = tsneDimReduction(sentenceVectorsCentered)\n",
    "    \n",
    "    data['sentenceVecDim1'] = sentenceVectors_TSNE[:,0]\n",
    "    data['sentenceVecDim2'] = sentenceVectors_TSNE[:,1]\n",
    "    data['sentenceVecCenteredDimCenter1'] = sentenceVectorsCentered_TSNE[:,0]\n",
    "    data['sentenceVecCenteredDimCenter2'] = sentenceVectorsCentered_TSNE[:,1]\n",
    "    \n",
    "    return data\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: computeSentenceVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = computeSentenceVectors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp -i ~/Desktop/alexguo.pem ubuntu@ec2-52-87-42-119.compute-1.amazonaws.com:/home/ubuntu/wikipedia/Visualization/test__sentenceVec.csv ~/Documents/UNC/Research\n"
     ]
    }
   ],
   "source": [
    "to_csv(newdata,'test__sentenceVec.csv', drop_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
