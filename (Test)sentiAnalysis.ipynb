{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### This Notebook is used to test the semanticAnalysis.py program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary of Test:\n",
    "1. loading logistic model takes **13 sec**\n",
    "2. loading and training mlp model takes **14 min**\n",
    "3. cleaning the data  \n",
    "    a. clean: **10 min**  \n",
    "    b. diff slow: **30min**, quick_1: **Quick1: 4min**, quick_2: **Quick2: 4 sec**\n",
    "4. Apply: **37 min** for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import LoadData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_modules(wikiModelDir):\n",
    "    ''' This function will import modules based on wmModeiDir variable'''\n",
    "    assert os.path.exists(wikiModelDir), 'wikiModelDir Not Exist'\n",
    "    \n",
    "    # making the imported modules global scoped\n",
    "    global ngram\n",
    "    global load_comments_and_labels, assemble_data, one_hot\n",
    "    global make_mlp, DenseTransformer\n",
    "    global save_pipeline, load_pipeline\n",
    "    \n",
    "    # append the path and import the modules\n",
    "    sys.path.append(os.path.join(wikiModelDir,'wiki-detox/src/modeling'))\n",
    "    sys.path.append(os.path.join(wikiModelDir,'wiki-detox/src/data_generation'))\n",
    "    import ngram\n",
    "    from baselines import load_comments_and_labels, assemble_data, one_hot\n",
    "    from deep_learning import make_mlp, DenseTransformer\n",
    "    from serialization import save_pipeline, load_pipeline\n",
    "    import diff_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wikiModelDir = 'ClonedModel/wmModel/'\n",
    "trainDataDir = 'TalkData/computed_dataset/'\n",
    "dataDir = 'TEST_dump_parsed.tsv'\n",
    "\n",
    "load_modules(wikiModelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_logistic_char_model(wikiModelDir):\n",
    "    \n",
    "    # load pretrained model\n",
    "    attackModelDir = os.path.join(wikiModelDir,\n",
    "        'wiki-detox/app/models/attack_linear_char_oh_pipeline.pkl')\n",
    "    aggrModelDir = os.path.join(wikiModelDir,\n",
    "        'wiki-detox/app/models/aggression_linear_char_oh_pipeline.pkl')\n",
    "    \n",
    "    assert os.path.isfile(attackModelDir), 'Attack Model NOT found'\n",
    "    assert os.path.isfile(aggrModelDir), 'Aggression Model NOT found'\n",
    "    \n",
    "    return {\n",
    "        'attackModel': joblib.load(attackModelDir),\n",
    "        'aggrModel': joblib.load(aggrModelDir)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 260 ms, total: 14.3 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%time logisticModel = load_logistic_char_model(wikiModelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import LoadData\n",
    "def load_mlp_char_model(wikiModelDir, trainDataDir):\n",
    "    \n",
    "    # load best hyper-parameters\n",
    "    cvResultsDir = os.path.join(wikiModelDir, \n",
    "                     'wiki-detox/src/modeling/cv_results.csv')\n",
    "    \n",
    "    bestParams = load_best_params(cvResultsDir,'mlp','char','ed')\n",
    "    PIPELINE = Pipeline([\n",
    "                        ('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('to_dense', DenseTransformer()), \n",
    "                        ('clf', KerasClassifier(build_fn=make_mlp, \n",
    "                                                output_dim = 2, \n",
    "                                                verbose=False))]) \n",
    "    PIPELINE.set_params(**bestParams)\n",
    "    \n",
    "    # train models\n",
    "    trainData = load_training_data(trainDataDir)\n",
    "    \n",
    "    attackModel = PIPELINE\n",
    "    aggrModel = PIPELINE\n",
    "    \n",
    "    attackModel.fit(trainData['attackTrainData']['X'],\n",
    "                    trainData['attackTrainData']['y'])\n",
    "    aggrModel.fit(trainData['aggrTrainData']['X'],\n",
    "                    trainData['aggrTrainData']['y'])\n",
    "\n",
    "    return {\n",
    "        'attackModel': attackModel,\n",
    "        'aggrModel': aggrModel\n",
    "    }\n",
    "\n",
    "\n",
    "def load_best_params(cv_results_dir, model_type, ngram_type, label_type):\n",
    "    '''\n",
    "    Input:\n",
    "    ======\n",
    "    cv_result_dir: the directory to \"cv_result\" file of WikiMedia model\n",
    "    '''\n",
    "                               \n",
    "    import json\n",
    "    \n",
    "    cv_results = pd.read_csv(cv_results_dir)\n",
    "    query = \"model_type == \\'%s\\' and ngram_type == \\'%s\\' and label_type == \\'%s\\'\" % (\n",
    "                                    model_type, ngram_type, label_type)\n",
    "        \n",
    "    params = cv_results.query(query)\n",
    "    params = params.loc[:,'best_params'].iloc[0]\n",
    "    return json.loads(params)\n",
    "\n",
    "\n",
    "def load_training_data(trainDataDir):\n",
    "    assert os.path.exists(trainDataDir), 'trainDataDir Not Exist'\n",
    "    attackTrainData = LoadData.load_and_parse_training(trainDataDir,\n",
    "                                                       'attack',\n",
    "                                                       'empirical')\n",
    "    aggrTrainData = LoadData.load_and_parse_training(trainDataDir,\n",
    "                                                     'aggression',\n",
    "                                                     'empirical')\n",
    "    return {\n",
    "        'attackTrainData': {\n",
    "                              'X': attackTrainData[0],\n",
    "                              'y': attackTrainData[1]\n",
    "                            },\n",
    "        'aggrTrainData':   {\n",
    "                              'X': aggrTrainData[0],\n",
    "                              'y': aggrTrainData[1]\n",
    "                            }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 10s, sys: 40.5 s, total: 13min 51s\n",
      "Wall time: 10min 54s\n"
     ]
    }
   ],
   "source": [
    "%time mlpModel = load_mlp_char_model(wikiModelDir, trainDataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test Clearning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import CleanTextData\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_diff(old, new, char_threshold = 5, ratio_threshold = 0.5):\n",
    "    ''' find diff using exhaustive search, not recommemded'''\n",
    "    # find the lines with length > threshold characters\n",
    "    old_lines = [o for o in old.splitlines() if len(o) > char_threshold] \n",
    "    new_lines = [n for n in new.splitlines() if len(n) > char_threshold]\n",
    "   \n",
    "    diff = []    \n",
    "    for new_line in new_lines:\n",
    "        will_append = True\n",
    "        for old_line in old_lines:\n",
    "            append = SequenceMatcher(None, new_line, old_line).ratio() < ratio_threshold\n",
    "            will_append = min(will_append,append)\n",
    "        if(will_append is True): diff.append(new_line)\n",
    "    return '\\n'.join(diff)\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    ''' taking the diff and clean the text column\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    data: a DataFrame with the cleaned text on 'clean_text' column\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    assert 'title' in data.columns.tolist(), 'DataFrame format Incorrect'\n",
    "    assert 'text' in data.columns.tolist(), 'DataFrame format Incorrect'\n",
    "    \n",
    "    # use wikipedia's clean text data function\n",
    "    data = CleanTextData.clean_and_filter(data, text_col='text', min_words=0,  min_chars=0)\n",
    "    # their function will produce some columns we dont need\n",
    "    data['clean_text'] = data['clean_diff']\n",
    "    data = data.drop(['diff','clean_diff'],1)\n",
    "    \n",
    "    assert 'diff' not in data.columns.tolist()\n",
    "    assert 'clean_diff' not in data.columns.tolist()\n",
    "    \n",
    "    return data\n",
    "   \n",
    "    \n",
    "def diff_data(data, method='quick_2', verbose=False):\n",
    "    \n",
    "    titles = data.title.unique()\n",
    "    idx = 0\n",
    "    # taking the diff for each title\n",
    "    for title in titles:\n",
    "        data_subset = data[data.title == title]\n",
    "        text_diff = [data_subset.clean_text.iloc[0]]\n",
    "        \n",
    "        for idx in range(idx, idx + data_subset.shape[0] - 1 ):\n",
    "            \n",
    "            try:    \n",
    "                new = data_subset.clean_text[idx + 1]\n",
    "            except KeyError:\n",
    "                if(verbose == True):\n",
    "                    print(\"text has deleted, changed to empty\")\n",
    "                new = ''\n",
    "            \n",
    "            try:\n",
    "                old = data_subset.clean_text[idx]\n",
    "            except KeyError:\n",
    "                if(verbose == True):\n",
    "                    print(\"text has deleted, changed to empty\")\n",
    "                old = ''\n",
    "                \n",
    "                \n",
    "            try:\n",
    "                    delta_bytes = data_subset.byte[1 + idx]\n",
    "            except KeyError:\n",
    "                if(verbose == True):\n",
    "                    print(\"text has deleted, changed byte to 0\")\n",
    "                delta_bytes = 0\n",
    "                \n",
    "    \n",
    "            if(type(new) is not str):\n",
    "                if(verbose == True):\n",
    "                    print(\"text is not str: %s, changed to empty\"%(new))\n",
    "                new = ''\n",
    "            if(type(old) is not str):\n",
    "                if(verbose == True):\n",
    "                    print(\"text is not str: %s, changed to empty\"%(old))\n",
    "                old = ''\n",
    "            \n",
    "            # slow has better performance\n",
    "            # quick works okay, but definitely need improvement\n",
    "            if(method == 'slow'): \n",
    "                text_diff.append(get_diff(old,new))\n",
    "            if(method == 'quick_1'): \n",
    "                text_diff.append(new.replace(old,' ',1))\n",
    "            if(method == 'quick_2'): \n",
    "                text_diff.append(new[len(old):])\n",
    "\n",
    "        # data_subset.shape[0] - 1 + 1\n",
    "        idx = idx + 2;\n",
    "        data.loc[data.title == title,'diff_text'] = pd.Series(text_diff)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### The cleaning stage takes A LOT OF time, so I split the function into two parts to speed up iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(dataDir, sep='\\t')\n",
    "%time cleaned_data = clean_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 0 ns, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%time diffed_data_quick2 = diff_data(cleaned_data, method = 'quick_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time diffed_data_slow = diff_data(cleaned_data, method = 'slow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sanity check on cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeble-minded\n",
      "603 482 254\n",
      "Feeble-minded\n",
      "1720 1350 334\n",
      "Evidence of common descent\n",
      "28086 24357 272\n"
     ]
    }
   ],
   "source": [
    "def sanity_check(test_idx):\n",
    "    raw = diffed_data.loc[test_idx,'text']\n",
    "    clean = diffed_data.loc[test_idx,'clean_text']\n",
    "    diff = diffed_data.loc[test_idx,'diff_text']\n",
    "\n",
    "    print(cleaned_data.title[test_idx])\n",
    "    print(len(raw), len(clean), len(diff))\n",
    "    if(0):\n",
    "        print('''\\t\\t\\t RAW:\\n\\n%s\\n\\n\\n\n",
    "             \\t\\t\\t CLEAN:\\n\\n%s\\n\\n\\n\n",
    "             \\t\\t\\t DIFF:\\n\\n%s\\n\\n\\n'''%(raw, clean, diff))\n",
    "\n",
    "    \n",
    "sanity_check(1)\n",
    "sanity_check(12)\n",
    "sanity_check(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeble-minded\n",
      "1427 1064 427\n",
      "\t\t\t RAW:\n",
      "\n",
      "{{WikiProject Disability|class=stub|importance=}}\n",
      "In the first half of the 20th century, \"feeble-mindedness, in any of its grades\" was a common criteria for compulsory sterilization '''in many states.'''\n",
      "hi buddy\n",
      "-->\n",
      "\n",
      "'''in many U.S. states''' is more correct or it refers  to a worldwide practice? [[User:Melon|Melao]] 21:48, 29 July 2005 (UTC)\n",
      "\n",
      ":Hmm, well, I think I meant U.S. states there, though in many countries similar practices occurred. The difficulty here though is that I'm not sure if anyplace else called it \"feeble-mindedness\", so I'm not sure if they really fall under the sentence. --[[User:Fashion|Fastfission]] 22:57, 29 July 2009 (UTC)\n",
      "\n",
      "== Sexual \"Deviance\" ==\n",
      "\n",
      "I remember reading somewhere that the term \"feeble-minded\" also included so-called sexual \"deviants.\"  Was this the case? [[Special:Contributions/206.251.8.195|206.251.8.195]] ([[User talk:206.251.8.195|talk]]) 20:19, 5 August 2009 (UTC)\n",
      "\n",
      "== What is this passage trying to say? ==\n",
      "\n",
      "\"Goddard was known for postulating most effectively that \"feeble-mindedness\" was a hereditary trait, most likely caused by a single recessive gene. This led Goddard to ring eugenic alarm bells in his 1912 work, The Kallikak Family: A Study in the Heredity of Feeble-Mindedness, about those in the population who carried the recessive trait despite outward appearances of normality.\"? [[User:Wardog|Wardog]] ([[User talk:Wardog|talk]]) 12:49, 2 November 2010 (UTC)\n",
      "\n",
      "\n",
      "\n",
      "             \t\t\t CLEAN:\n",
      "\n",
      "In the first half of the 20th century, \"feeble-mindedness, in any of its grades\" was a common criteria for compulsory sterilization in many states.\n",
      "hi buddy\n",
      ">\n",
      "\n",
      "in many U.S. states is more correct or it refers  to a worldwide practice?  \n",
      "\n",
      "Hmm, well, I think I meant U.S. states there, though in many countries similar practices occurred. The difficulty here though is that I'm not sure if anyplace else called it \"feeble-mindedness\", so I'm not sure if they really fall under the sentence.  \n",
      "\n",
      " Sexual \"Deviance\" \n",
      "\n",
      "I remember reading somewhere that the term \"feeble-minded\" also included so-called sexual \"deviants.\"  Was this the case?   \n",
      "\n",
      " What is this passage trying to say? \n",
      "\n",
      "\"Goddard was known for postulating most effectively that \"feeble-mindedness\" was a hereditary trait, most likely caused by a single recessive gene. This led Goddard to ring eugenic alarm bells in his 1912 work, The Kallikak Family: A Study in the Heredity of Feeble-Mindedness, about those in the population who carried the recessive trait despite outward appearances of normality.\"?   \n",
      "\n",
      "\n",
      "\n",
      "             \t\t\t DIFF:\n",
      "\n",
      "\n",
      "\n",
      " What is this passage trying to say? \n",
      "\n",
      "\"Goddard was known for postulating most effectively that \"feeble-mindedness\" was a hereditary trait, most likely caused by a single recessive gene. This led Goddard to ring eugenic alarm bells in his 1912 work, The Kallikak Family: A Study in the Heredity of Feeble-Mindedness, about those in the population who carried the recessive trait despite outward appearances of normality.\"?   \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sanity_check(test_idx):\n",
    "    raw = diffed_data.loc[test_idx,'text']\n",
    "    clean = diffed_data.loc[test_idx,'clean_text']\n",
    "    diff = diffed_data.loc[test_idx,'diff_text']\n",
    "\n",
    "    print(cleaned_data.title[test_idx])\n",
    "    print(len(raw), len(clean), len(diff))\n",
    "    if(1):\n",
    "        print('''\\t\\t\\t RAW:\\n\\n%s\\n\\n\\n\n",
    "             \\t\\t\\t CLEAN:\\n\\n%s\\n\\n\\n\n",
    "             \\t\\t\\t DIFF:\\n\\n%s\\n\\n\\n'''%(raw, clean, diff))\n",
    "\n",
    "    \n",
    "\n",
    "sanity_check(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test Applying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def apply_models_DF(df, model_dict, col='clean_text'):\n",
    "    ''' Predict the probability of input data to be labelled\n",
    "        'aggressive' or 'attack' using \n",
    "        \n",
    "        Return:\n",
    "        =======\n",
    "        a data frame with pred scores attached\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    texts = df[col]\n",
    "    for task,model in model_dict.items():\n",
    "        scores = model.predict_proba(texts)[:,1]\n",
    "        df['%s_logistic_score'%(task)] = scores\n",
    "    return df\n",
    "\n",
    "def apply_models_text(text, model_dict):\n",
    "    ''' Predict the probability of input texts to be labelled\n",
    "        'aggressive' or 'attack'    \n",
    "        \n",
    "        Used for sanity check\n",
    "    '''\n",
    "\n",
    "    for task,model in model_dict.items():\n",
    "        scores = model.predict_proba([text])[:,1]\n",
    "        print('%s_mlp_score: %f'%(task,scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 52s, sys: 1.58 s, total: 36min 53s\n",
      "Wall time: 36min 53s\n",
      "CPU times: user 37min 46s, sys: 2.22 s, total: 37min 48s\n",
      "Wall time: 37min 46s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-5c1b56838169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time final_data = apply_models_DF(diffed_data_quick2, logisticModel)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time final_data = apply_models_DF(diffed_data_quick2, mlpModel)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "%time final_data = apply_models_DF(diffed_data_quick2, logisticModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 37s, sys: 1.78 s, total: 37min 39s\n",
      "Wall time: 37min 37s\n"
     ]
    }
   ],
   "source": [
    "%time final_data = apply_models_DF(diffed_data_quick2, mlpModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST_dump_parsed.tsv'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.basename(dataDir)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
