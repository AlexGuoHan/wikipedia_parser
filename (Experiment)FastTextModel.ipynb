{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Experimenting FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('ClonedModel/wmModel/wiki-detox/src/modeling/')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import joblib\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some Helper Functions with WikiMedia\n",
    "import numpy as np\n",
    "\n",
    "def empirical_dist(l, w = 0.0, index = None):\n",
    "    \"\"\"\n",
    "    Compute empirical distribution over all classes\n",
    "    using all labels with the same rev_id\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        index = sorted(list(set(l.dropna().values)))\n",
    "\n",
    "    data = {}\n",
    "    for k, g in l.groupby(l.index):\n",
    "        data[k] = g.value_counts().reindex(index).fillna(0) + w\n",
    "\n",
    "    labels = pd.DataFrame(data).T\n",
    "    labels = labels.fillna(0)\n",
    "    labels = labels.div(labels.sum(axis=1), axis=0)\n",
    "    return labels\n",
    "\n",
    "def plurality(l):\n",
    "    \"\"\"\n",
    "    Take the most common label from all labels with the same rev_id.\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    s = an array of integers of 0 or 1\n",
    "    \"\"\"\n",
    "    s = l.groupby(l.index).apply(lambda x:x.value_counts().index[0])\n",
    "    s.name = 'y'\n",
    "    return s\n",
    "\n",
    "def one_hot(y):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    =======\n",
    "    y_oh = an array of vectors (one-hot vectors)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    if len(y.shape) == 1:\n",
    "        n = len(set(y.ravel()))\n",
    "        idxs = y.astype(int)\n",
    "    else:\n",
    "        idxs = y.argmax(axis = 1)\n",
    "        n = y.shape[1]\n",
    "\n",
    "    y_oh = np.zeros((m, n))\n",
    "    y_oh[list(range(m)), idxs] = 1\n",
    "    return y_oh\n",
    "\n",
    "def load_and_parse_training(data_dir, task, data_type):\n",
    "    COMMENTS_FILE = \"%s_annotated_comments.tsv\" % task\n",
    "    LABELS_FILE = \"%s_annotations.tsv\" % task\n",
    "    comments = pd.read_csv(os.path.join(data_dir, COMMENTS_FILE), sep = '\\t', index_col = 0)\n",
    "    # remove special newline and tab tokens\n",
    "\n",
    "    comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "    comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "\n",
    "    annotations = pd.read_csv(os.path.join(data_dir, LABELS_FILE),  sep = '\\t', index_col = 0)\n",
    "    \n",
    "    X = comments.sort_index()['comment'].values\n",
    "    \n",
    "    if(data_type == 'empirical'):\n",
    "        labels = empirical_dist(annotations[task])\n",
    "        y = labels.sort_index().values        \n",
    "    elif(data_type == 'onehot'):\n",
    "        y = plurality(annotations[task])\n",
    "        \n",
    "    assert(X.shape[0] == y.shape[0])    \n",
    "    return X, y  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 460 ms, total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "DATA_DIR = 'TalkData/computed_dataset/'\n",
    "task = 'attack'\n",
    "%time [X,yEmp] = load_and_parse_training(DATA_DIR, task, 'empirical')\n",
    "yOneHot = one_hot(yEmp)\n",
    "\n",
    "\n",
    "# [_,yPlurality] = load_and_parse_training(DATA_DIR, task, 'onehot') will disagree with yOneHot when prob = 0.5 ()\n",
    "# this occurs 0.869%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, yOneHot, \n",
    "                                    test_size=0.15, \n",
    "                                    random_state=0\n",
    "                                    )\n",
    "\n",
    "\n",
    "DATA_TRAINING = pd.DataFrame([X_train,y_train]).T\n",
    "DATA_TESTING = pd.DataFrame([X_test,y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_TRAINING.columns = ['Text','Category']\n",
    "DATA_TRAINING['Category'] = DATA_TRAINING['Category'].apply(\n",
    "                lambda y: 'notAttack' if y.argmax() == 0 else 'Attack')\n",
    "\n",
    "DATA_TESTING.columns = ['Text','Category']\n",
    "DATA_TESTING['Category'] = DATA_TESTING['Category'].apply(\n",
    "                lambda y: 'notAttack' if y.argmax() == 0 else 'Attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_dir = 'ATTACK_TRAINING.txt'\n",
    "TESTING_DATA_dir = 'ATTACK_TESING.txt'\n",
    "\n",
    "\n",
    "with open(TRAINING_DATA_dir, 'w') as f:\n",
    "    for row in DATA_TRAINING.iterrows():\n",
    "        text = row[1]['Text']\n",
    "        category = row[1]['Category']\n",
    "        f.write('{}. __label__{}\\n'.format(text, category))\n",
    "with open(TESTING_DATA_dir, 'w') as f:\n",
    "    for row in DATA_TESTING.iterrows():\n",
    "        text = row[1]['Text']\n",
    "        category = row[1]['Category']\n",
    "        f.write('{}. __label__{}\\n'.format(text, category))\n",
    "\n",
    "# make sure they are different (reduce human error)\n",
    "with open(TRAINING_DATA_dir, 'r') as f:\n",
    "    training = f.readlines()\n",
    "with open(TESTING_DATA_dir, 'r') as f:\n",
    "    testing = f.readlines()\n",
    "    \n",
    "assert len(training) != len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training and Evaluating FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "import subprocess\n",
    "FASTTEXT_dir = 'ClonedModel/fastText/'\n",
    "assert(os.path.exists(FASTTEXT_dir))\n",
    "\n",
    "OUTPUT_MODEL_dir = 'model'\n",
    "subprocess.call(\"sudo {}./fasttext supervised -input {} -output {} -epoch 2\".format(FASTTEXT_dir,\n",
    "                                                                          TRAINING_DATA_dir,\n",
    "                                                                          OUTPUT_MODEL_dir),\n",
    "               shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "PRED_FILE_dir = 'PREDICTED.txt'\n",
    "subprocess.call(\"sudo {}./fasttext predict {}.bin {} > {}\".format(FASTTEXT_dir,\n",
    "                                                        OUTPUT_MODEL_dir,\n",
    "                                                        TESTING_DATA_dir,\n",
    "                                                        PRED_FILE_dir),\n",
    "               shell=True)\n",
    "subprocess.call(\"sudo {}./fasttext test {}.bin {}\".format(FASTTEXT_dir,\n",
    "                                                        OUTPUT_MODEL_dir,\n",
    "                                                        TESTING_DATA_dir),\n",
    "               shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the roc_scorer and spearman_scorer from WikiMedia\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def expectation(y):\n",
    "    classes = np.arange(y.shape[1])\n",
    "    return y.dot(classes)\n",
    "\n",
    "def multi_class_roc_auc(true, pred, average = 'macro'):\n",
    "    true = one_hot(true)\n",
    "    #print(true)\n",
    "    return roc_auc_score(true, pred, average = average)\n",
    "\n",
    "def multi_class_spearman(true, pred):\n",
    "    return spearmanr(expectation(true), expectation(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(PRED_FILE_dir, 'r') as f:\n",
    "    preds = f.readlines()\n",
    "preds = [p.strip().strip('__label__') for p in preds]\n",
    "preds = [[1, 0] if p == 'notAttack' else [0, 1] for p in preds]\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC:\n",
      " \t0.7772895349049003\n",
      "SPEARMAN:\n",
      " \tSpearmanrResult(correlation=0.65849963956608937, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spearman = multi_class_spearman(y_test, preds)\n",
    "roc = multi_class_roc_auc(y_test, preds)\n",
    "print(\"ROC:\\n \\t{}\".format(roc))\n",
    "print(\"SPEARMAN:\\n \\t{}\".format(spearman))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions  \n",
    "**FastText** performs worse on this dataset. The spearman score is similar, but the roc_auc is significantly worse. I suspect this is due to fact that the because the predictions are categorical not real-valued. Still, the use of FastText have no significant value except being extremely fast (way less than 1s using FastText, a few seconds using charCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
