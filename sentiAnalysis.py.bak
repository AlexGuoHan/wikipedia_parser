import sys, os
import joblib
import pandas as pd


from bs4 import BeautifulSoup
from argparse import ArgumentParser
from difflib import SequenceMatcher
from sklearn.pipeline import Pipeline
from multiprocessing import Pool, cpu_count
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from keras.wrappers.scikit_learn import KerasClassifier


import LoadData
import CleanTextData


class Stopwatch:
    start_time=None
    def go(self,msg=''):
        if msg:
            print(msg),
        self.start_time=time.time()
        sys.stdout.flush()
    def stop(self,msg=''):
        if msg:
            print("{}: {} seconds".format(msg,time.time()-self.start_time))
        else:
            print("Elapsed time: {} seconds".format(time.time()-self.start_time))
        sys.stdout.flush()
    def check(self):
        return time.time()-self.start_time
tic=Stopwatch()


def argParser():
    parser = ArgumentParser()
    parser.add_argument('--wikiModelDir', type=str,
                        dest='wikiModelDir', 
                        help='directory to wikiMedia models',
                        required=True)
    parser.add_argument('--trainDataDir', type=str, 
                        dest='trainDataDir', 
                        help='directory to training data', 
                        required=True)
    parser.add_argument('--dataFileDir', type=str, 
                        dest='dataDir', 
                        help='directory to data to be predicted', 
                        required=True)
    parser.add_argument('--cpu', type=int,
                        dest='cpu',
                        help='number of cpu to deploy, 0 for max',
                        required=True)
    return parser;



def main():
    parser = argParser()
    args = parser.parse_args()
    
    wikiModelDir = args.wikiModelDir
    trainDataDir = args.trainDataDir
    dataFileDir = args.dataFileDir
    num_cpus = args.cpu
    
    # load modules
    load_modules(wikiModelDir)
    
    tic.go('LOADING MODELS')
    
    # load losgistic model
    logisticModel = load_logistic_char_model(wikiModelDir)
    
    # load and train mlp model
    mlpModel = load_mlp_char_model(wikiModelDir, trainDataDir)
    
    tic.stop()
    
    # parallelize over multiple cpu
    with open(dataFileDir,'r') as f:
            dataFiles = f.readlines()
    assert cpu_count() >= num_cpus,'more cpu than available'
    if(cpu_count <= 0): num_cpus = cpu_counts()
    print('CPU: %d' % num_cpus)
    
    pool = Pool(num_cpus)
    pool.map(load_apply_save,dataFiles)
    pool.join()
    pool.close()


    

def load_apply_save(dataDir):
    
    # load data
    tic.go('LOADING & CLEANING DATA %s'%(dataDir))
    raw_data = pd.read_csv(dataDir, sep='\t')
    cleaned_data = diff_and_clean(raw_data)
    tic.stop()
    
    # apply two models
    tic.go('APPLYING MODELS')
    cleaned_data = apply_models_DF(cleaned_data, logisticModel)
    cleaned_data = apply_models_DF(cleaned_data, mlpModel)
    tic.stop()
    
    # save
    filename = os.path.basename(dataDir)
    data.to_csv('predicted_%s'%filename)
    
    
    
    

def load_modules(wikiModelDir):
    ''' This function will import modules based on wmModeiDir variable'''
    assert os.path.exists(wikiModelDir), 'wikiModelDir Not Exist'
    
    global ngram
    global load_comments_and_labels, assemble_data, one_hot
    global make_mlp, DenseTransformer
    global save_pipeline, load_pipeline
    global diff_utils
    
    sys.path.append(os.path.join(wikiModelDir,'wiki-detox/src/modeling'))
    sys.path.append(os.path.join(wikiModelDir,'wiki-detox/src/data_generation'))
    import ngram
    from baselines import load_comments_and_labels, assemble_data, one_hot
    from deep_learning import make_mlp, DenseTransformer
    from serialization import save_pipeline, load_pipeline
    import diff_utils
    
    

def load_logistic_char_model(wikiModelDir):
    ''' Load and return the pretrained logistic character module '''
    
    # load pretrained model
    attackModelDir = os.path.join(wikiModelDir,
        'wiki-detox/app/models/attack_linear_char_oh_pipeline.pkl')
    aggrModelDir = os.path.join(wikiModelDir,
        'wiki-detox/app/models/aggression_linear_char_oh_pipeline.pkl')
    
    assert os.path.isfile(attackModelDir), 'Attack Model NOT found'
    assert os.path.isfile(aggrModelDir), 'Aggression Model NOT found'
    
    return {
        'attackModel': joblib.load(attackModelDir),
        'aggrModel': joblib.load(aggrModelDir)
    }
    
    
def load_mlp_char_model(wikiModelDir, trainDataDir):
    
    # load best hyper-parameters
    cvResultsDir = os.path.join(wikiModelDir, 
                     'wiki-detox/src/modeling/cv_results.csv')
    
    bestParams = load_best_params(cvResultsDir,'mlp','char','ed')
    PIPELINE = Pipeline([
                        ('vect', CountVectorizer()),
                        ('tfidf', TfidfTransformer()),
                        ('to_dense', DenseTransformer()), 
                        ('clf', KerasClassifier(build_fn=make_mlp, 
                                                output_dim = 2, 
                                                verbose=False))]) 
    PIPELINE.set_params(**bestParams)
    
    # train models
    trainData = load_training_data(trainDataDir)
    
    attackModel = PIPELINE
    aggrModel = PIPELINE
    
    attackModel.fit(trainData['attackTrainData']['X'],
                    trainData['attackTrainData']['y'])
    aggrModel.fit(trainData['aggrTrainData']['X'],
                    trainData['aggrTrainData']['y'])

    return {
        'attackModel': attackModel,
        'aggrModel': aggrModel
    }


def load_best_params(cv_results_dir, model_type, ngram_type, label_type):
    '''
    Input:
    ======
    cv_result_dir: the directory to "cv_result" file of WikiMedia model
    '''
                               
    import json
    
    cv_results = pd.read_csv(cv_results_dir)
    query = "model_type == \'%s\' and ngram_type == \'%s\' and label_type == \'%s\'" % (
                                    model_type, ngram_type, label_type)
        
    params = cv_results.query(query)
    params = params.loc[:,'best_params'].iloc[0]
    return json.loads(params)


def load_training_data(trainDataDir):
    assert os.path.exists(trainDataDir), 'trainDataDir Not Exist'
    attackTrainData = LoadData.load_and_parse_training(trainDataDir,
                                                       'attack',
                                                       'empirical')
    aggrTrainData = LoadData.load_and_parse_training(trainDataDir,
                                                     'aggression',
                                                     'empirical')
    return {
        'attackTrainData': {
                              'X': attackTrainData[0],
                              'y': attackTrainData[1]
                            },
        'aggrTrainData':   {
                              'X': aggrTrainData[0],
                              'y': aggrTrainData[1]
                            }
    }
                               

def get_diff(old, new, char_threshold = 5, ratio_threshold = 0.5):
    ''' find diff using exhaustive search, not recommemded'''
    # find the lines with length > threshold characters
    old_lines = [o for o in old.splitlines() if len(o) > char_threshold] 
    new_lines = [n for n in new.splitlines() if len(n) > char_threshold]
   
    diff = []    
    for new_line in new_lines:
        will_append = True
        for old_line in old_lines:
            append = SequenceMatcher(None, new_line, old_line).ratio() < ratio_threshold
            will_append = min(will_append,append)
        if(will_append is True): diff.append(new_line)
    return '\n'.join(diff)


def clean_data(data, method='quick_2', verbose=False):
    ''' taking the diff and clean the text column
    
    Return:
    =======
    data: a DataFrame with the cleaned text on 'clean_text' column
    
    '''
    
    # Clean the data
    assert 'title' in data.columns.tolist(), 'DataFrame format Incorrect'
    assert 'text' in data.columns.tolist(), 'DataFrame format Incorrect'
    
    # use wikipedia's clean text data function
    data = CleanTextData.clean_and_filter(data, text_col='text', min_words=0,  min_chars=0)
    
    # their function will produce some columns we dont need
    data['clean_text'] = data['clean_diff']
    data = data.drop(['diff','clean_diff'],1)
    
    assert 'diff' not in data.columns.tolist()
    assert 'clean_diff' not in data.columns.tolist()
    

    
    
    # Diff the data
    titles = data.title.unique()
    idx = 0
    # taking the diff for each title
    for title in titles:
        data_subset = data[data.title == title]
        text_diff = [data_subset.clean_text.iloc[0]]
        
        for idx in range(idx, idx + data_subset.shape[0] - 1 ):
            
            try:    
                new = data_subset.clean_text[idx + 1]
            except KeyError:
                if(verbose == True):
                    print("text has deleted, changed to empty")
                new = ''
            
            try:
                old = data_subset.clean_text[idx]
            except KeyError:
                if(verbose == True):
                    print("text has deleted, changed to empty")
                old = ''
                
                
            try:
                    delta_bytes = data_subset.byte[1 + idx]
            except KeyError:
                if(verbose == True):
                    print("text has deleted, changed byte to 0")
                delta_bytes = 0
                
    
            if(type(new) is not str):
                if(verbose == True):
                    print("text is not str: %s, changed to empty"%(new))
                new = ''
            if(type(old) is not str):
                if(verbose == True):
                    print("text is not str: %s, changed to empty"%(old))
                old = ''
            
            # slow has better performance
            # quick works okay, but definitely need improvement
            if(method == 'slow'): 
                text_diff.append(get_diff(old,new))
            if(method == 'quick_1'): 
                text_diff.append(new.replace(old,' ',1))
            if(method == 'quick_2'): 
                text_diff.append(new[len(old):])

        # data_subset.shape[0] - 1 + 1
        idx = idx + 2;
        data.loc[data.title == title,'diff_text'] = pd.Series(text_diff)
    
    return data
                               
    
def apply_models_DF(df, model_dict, col='clean_text'):
    ''' Predict the probability of input data to be labelled
        'aggressive' or 'attack' using 
        
        Return:
        =======
        a data frame with pred scores attached
        
    '''
    
    texts = df[col]
    for task,model in model_dict.items():
        scores = model.predict_proba(texts)[:,1]
        df['%s_logistic_score'%(task)] = scores
    return df

def apply_models_text(text, model_dict):
    ''' Predict the probability of input texts to be labelled
        'aggressive' or 'attack'    
        
        Used for sanity check
    '''

    for task,model in model_dict.items():
        scores = model.predict_proba([text])[:,1]
        print('%s_mlp_score: %f'%(task,scores))
    
    
    
    
    
    
if __name__ == '__main__':
    main()
