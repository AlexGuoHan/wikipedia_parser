{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Character CNN Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learn = tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "N_FILTERS = 10\n",
    "FILTER_SHAPE1 = [20, 256]\n",
    "FILTER_SHAPE2 = [20, N_FILTERS]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define a 3-layer convolutional neural network\n",
    "# 2 conv layers + 1 fully_connected\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "def char_cnn_model(features, target):\n",
    "    '''Character Level CNN to predict classes'''\n",
    "    # tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)\n",
    "    target = tf.one_hot(target, 15, 1, 0)\n",
    "    byte_list = tf.reshape(\n",
    "        # this transforms features into onehot vectors of 256 dimensions\n",
    "        tf.one_hot(features,256),\n",
    "        # reshape that into [n_samples, 256 length_per_sample, dim_per_char, 1]\n",
    "        [-1, MAX_DOCUMENT_LENGTH, 256, 1]\n",
    "    )\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('CNN_layer_1'):\n",
    "            conv1 = tf.contrib.layers.convolution2d(\n",
    "                                            byte_list, \n",
    "                                            N_FILTERS, \n",
    "                                            FILTER_SHAPE1, \n",
    "                                            padding='VALID')\n",
    "            # add RELU\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            # max pool\n",
    "            pool1 = tf.nn.max_pool(\n",
    "                                conv1,\n",
    "                                ksize=[1, POOLING_WINDOW, 1, 1],\n",
    "                                strides=[1, POOLING_STRIDE, 1, 1],\n",
    "                                padding='SAME')\n",
    "            # transpose the matrix so that n_filters becomes width\n",
    "            pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "        with tf.variable_scope('CNN_layer_2'):\n",
    "            conv2 = tf.contrib.layers.convolution2d(\n",
    "                                            pool1, \n",
    "                                            N_FILTERS, \n",
    "                                            FILTER_SHAPE2, \n",
    "                                            padding='VALID')\n",
    "            # max across each filter to get useful features for classification\n",
    "            # Reduces input_tensor along the dimensions given in reduction_indices. \n",
    "            # Unless keep_dims is true, the rank of the tensor is reduced by 1 \n",
    "            #     for each entry in reduction_indices. \n",
    "            # If keep_dims is true, the reduced dimensions are retained with length 1.\n",
    "\n",
    "            # tf.reduce_max reduce the [1] dimension \n",
    "            # tf.squeeze(input, axis=None, name=None, squeeze_dims=None)\n",
    "            #     squeeze_dims: Deprecated keyword argument that is now axis\n",
    "            #      this operation will have all dimensions of size 1 removed. \n",
    "            #      squeeze_dims / axis = [1] means squeezing the first size 1 dimension\n",
    "            pool2 = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims=[1])\n",
    "\n",
    "        # fully connected linear classifier of WX+B\n",
    "        logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)\n",
    "        loss = tf.losses.softmax_cross_entropy(target, logits)\n",
    "\n",
    "        # training op\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss,\n",
    "            tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=0.01)\n",
    "    \n",
    "    return ({\n",
    "        'class': tf.argmax(logits, 1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dbpediaExperiment():\n",
    "    dbpedia = learn.datasets.load_dataset(\n",
    "                'dbpedia', test_with_fake_data=False, size='small')\n",
    "    x_train = pd.DataFrame(dbpedia.train.data)[1]\n",
    "    y_train = pd.Series(dbpedia.train.target)\n",
    "    x_test = pd.DataFrame(dbpedia.test.data)[1]\n",
    "    y_test = pd.Series(dbpedia.test.target)\n",
    "\n",
    "    # process vocabulary\n",
    "    char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "    x_train = np.array(list(char_processor.fit_transform(x_train))) # shape = [n_sample, n_doc_length]\n",
    "    x_test = np.array(list(char_processor.fit_transform(x_test)))\n",
    "\n",
    "    classifier = learn.Estimator(model_fn = char_cnn_model)\n",
    "    classifier.fit(x_train, y_train, steps=1)\n",
    "    preds = classifier.predict(x_test, as_iterable=True)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = dbpediaExperiment();\n",
    "class_preds = [p['class'] for p in preds]\n",
    "prob_preds = [p['prob'] for p in preds]\n",
    "score = metrics.accuracy_score(y_test, class_preds)\n",
    "print('Accuracy:{}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Applying this to WikiTalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import Load_Data\n",
    "load_data = False\n",
    "\n",
    "if(load_data == True):\n",
    "    %time DATA = Load_Data.load_Onehot_train_test_split()\n",
    "    training = DATA['Training']\n",
    "    testing = DATA['Testing']\n",
    "else:\n",
    "    training = pd.read_csv('trainingWiki.csv')\n",
    "    testing = pd.read_csv('testingWiki.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild the Model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 200\n",
    "N_FILTERS = 10\n",
    "FILTER_SHAPE1 = [20, 256]\n",
    "FILTER_SHAPE2 = [20, N_FILTERS]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "N_CLASSES = 2\n",
    "N_STEPS = 30\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "def wiki_char_cnn_model(features, target):\n",
    "    '''Character Level CNN to predict classes'''\n",
    "    target = tf.one_hot(target, N_CLASSES, 1, 0)\n",
    "    byte_list = tf.reshape(\n",
    "        tf.one_hot(features,256),\n",
    "        [-1, MAX_DOCUMENT_LENGTH, 256, 1]\n",
    "    )\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('CNN_layer_1'):\n",
    "            conv1 = tf.contrib.layers.convolution2d(\n",
    "                                            byte_list, \n",
    "                                            N_FILTERS, \n",
    "                                            FILTER_SHAPE1, \n",
    "                                            padding='VALID')\n",
    "            # add RELU\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            # max pool\n",
    "            pool1 = tf.nn.max_pool(\n",
    "                                conv1,\n",
    "                                ksize=[1, POOLING_WINDOW, 1, 1],\n",
    "                                strides=[1, POOLING_STRIDE, 1, 1],\n",
    "                                padding='SAME')\n",
    "            # transpose the matrix so that n_filters becomes width\n",
    "            pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "        with tf.variable_scope('CNN_layer_2'):\n",
    "            conv2 = tf.contrib.layers.convolution2d(\n",
    "                                            pool1, \n",
    "                                            N_FILTERS, \n",
    "                                            FILTER_SHAPE2, \n",
    "                                            padding='VALID')\n",
    "    \n",
    "            pool2 = tf.squeeze(tf.reduce_max(conv2,1), squeeze_dims=[1])\n",
    "\n",
    "        # fully connected linear classifier of WX+B\n",
    "        logits = tf.contrib.layers.fully_connected(pool2, N_CLASSES, activation_fn=None)\n",
    "        loss = tf.losses.softmax_cross_entropy(target, logits)\n",
    "\n",
    "        # training op\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss,\n",
    "            tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=0.01)\n",
    "    \n",
    "    return ({\n",
    "        'class': tf.argmax(logits, 1),\n",
    "        'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wikiExperiment():\n",
    "    char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "    \n",
    "    train_x = np.array(list(char_processor.fit_transform(training.Text)))\n",
    "    train_y = training.Category.apply(lambda x: 0 if x=='Attack' else 1)\n",
    "    \n",
    "    test_x = np.array(list(char_processor.fit_transform(testing.Text)))\n",
    "    test_y = testing.Category.apply(lambda x: 0 if x=='Attack' else 1)\n",
    "    \n",
    "    classifier = learn.Estimator(model_fn = wiki_char_cnn_model)\n",
    "    \n",
    "    classifier.fit(train_x, train_y, steps=N_STEPS, batch_size=10)\n",
    "    wikiPreds = classifier.predict(test_x, as_iterable=True)\n",
    "    \n",
    "    test_y = testing.Category.apply(lambda x: 0 if x=='Attack' else 1)\n",
    "    class_preds = [p['class'] for p in wikiPreds]\n",
    "    prob_preds = [p['prob'] for p in wikiPreds]\n",
    "    score = metrics.accuracy_score(test_y, class_preds) # wrong class\n",
    "    print('Accuracy:{}'.format(score))\n",
    "    \n",
    "    return wikiPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmu4zlm7a\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_tf_random_seed': None, '_evaluation_master': '', '_master': '', '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_task_id': 0, '_keep_checkpoint_max': 5, '_environment': 'local', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbf266cf278>, '_task_type': None, '_is_chief': True, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:From <ipython-input-36-16f3dd3775e4>:12: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-36-16f3dd3775e4>:12: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-36-16f3dd3775e4>:12: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpmu4zlm7a/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 0.704781\n",
      "INFO:tensorflow:Saving checkpoints for 30 into /tmp/tmpmu4zlm7a/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.345497.\n",
      "WARNING:tensorflow:From <ipython-input-36-16f3dd3775e4>:13: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    }
   ],
   "source": [
    "# do not run this without GPU, \n",
    "# this is really expensive\n",
    "wikiPreds = wikiExperiment();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
